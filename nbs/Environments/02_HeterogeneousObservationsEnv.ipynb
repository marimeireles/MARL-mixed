{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da35e275-00e5-48fa-84f2-7f0721804e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp Environments/HeterogeneousObservationsEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f44aea3-a46f-4db1-8ec3-0780e679ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# Imports for the nbdev development environment\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de765b1-fae2-4d63-b275-deb7de1ad4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from fastcore.utils import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42dfe465-c409-4fcf-b40a-0c27724dd8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd103bb1-50f6-469d-a073-617a3dcd0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class HeterogeneousObservationsEnv(object):\n",
    "    def __init__(self, observation_type=\"default\", observation_value=None):\n",
    "        # Observation configurations\n",
    "        self.observation_type = observation_type\n",
    "        self.observation_value = observation_value\n",
    "\n",
    "        self.transitions = self.transition_tensor()\n",
    "        self.final_states = np.array(self.generate_final_states())\n",
    "        self.rewards = self.reward_tensor()\n",
    "        self.observations = self.generate_observation_tensor()\n",
    "\n",
    "        self.actions_set = self.actions()\n",
    "        self.states_set = self.states()\n",
    "        self.observation_set = self.generate_observation_set()\n",
    "\n",
    "        self.n_agents = self.rewards.shape[0]\n",
    "        self.n_states = self.transitions.shape[0]\n",
    "        self.n_agent_actions = self.transitions.shape[1]\n",
    "\n",
    "        self.n_possible_observations = self.n_states\n",
    "\n",
    "        # Checks\n",
    "        assert all(\n",
    "            dim == self.n_agent_actions for dim in self.rewards.shape[2:-1]\n",
    "        ), \"Inconsistent number of actions\"\n",
    "        assert len(self.actions_set) == self.n_agents and all(\n",
    "            len(a) == self.n_agent_actions for a in self.actions_set\n",
    "        ), \"Inconsistent number of actions\"\n",
    "        assert (\n",
    "            self.transitions.shape[-1] == self.n_states\n",
    "            and self.rewards.shape[-1] == self.n_states\n",
    "        ), \"Inconsistent number of states\"\n",
    "        assert self.rewards.shape[1] == self.n_states, \"Inconsistent number of states\"\n",
    "        assert len(self.final_states) == self.n_states, \"Inconsistent number of states\"\n",
    "        assert len(self.states_set) == self.n_states, \"Inconsistent number of states\"\n",
    "        assert np.allclose(\n",
    "            self.transitions.sum(-1), 1\n",
    "        ), \"Transition model probabilities do not sum to 1\"\n",
    "        assert (\n",
    "            self.observations.shape[0] == self.n_agents\n",
    "        ), \"Inconsistent number of agents\"\n",
    "        assert (\n",
    "            self.observations.shape[1] == self.n_states\n",
    "        ), \"Inconsistent number of states\"\n",
    "        assert np.allclose(\n",
    "            self.observations.sum(-1), 1\n",
    "        ), \"Observation model probabilities do not sum to 1\"\n",
    "\n",
    "        # Ensure naming compatibility with the rest of PyCRDT\n",
    "        self.R = self.rewards\n",
    "        self.N = self.n_agents\n",
    "        self.F = self.final_states\n",
    "        self.M = self.n_agent_actions\n",
    "        self.Z = self.n_states\n",
    "        self.T = self.transitions\n",
    "        self.O = self.observations\n",
    "        self.Oset = self.observation_set\n",
    "        self.Q = self.n_possible_observations\n",
    "        self.Aset = self.actions_set\n",
    "        self.Sset = self.states_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0a4ef-1c5c-424e-87bc-8b32930be2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def id(self: HeterogeneousObservationsEnv):\n",
    "    \"\"\"Returns id string of environment\"\"\"\n",
    "    return f\"{self.__class__.__name__}\"\n",
    "\n",
    "\n",
    "@patch\n",
    "def __str__(self: HeterogeneousObservationsEnv):\n",
    "    return self.id()\n",
    "\n",
    "\n",
    "@patch\n",
    "def __repr__(self: HeterogeneousObservationsEnv):\n",
    "    return self.id()\n",
    "\n",
    "\n",
    "@patch\n",
    "def transition_tensor(self: HeterogeneousObservationsEnv):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "@patch\n",
    "def reward_tensor(self: HeterogeneousObservationsEnv):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "@patch\n",
    "def generate_observation_tensor(self: HeterogeneousObservationsEnv):\n",
    "    self.n_possible_observations = self.n_states\n",
    "    # Initialize the observation tensor filled with zeros.\n",
    "    observations_iso = np.zeros(\n",
    "        (self.n_agents, self.n_states, self.n_possible_observations)\n",
    "    )\n",
    "\n",
    "    # Check if observation_type and observation_value are lists\n",
    "    if isinstance(self.observation_type, list) and isinstance(\n",
    "        self.observation_value, list\n",
    "    ):\n",
    "        for agent_index, (obs_type, focused_value) in enumerate(\n",
    "            zip(self.observation_type, self.observation_value)\n",
    "        ):\n",
    "            self._apply_observation_configuration(\n",
    "                agent_index, obs_type, focused_value, observations_iso\n",
    "            )\n",
    "    else:\n",
    "        # Handle the case where observation_type and observation_value are single values\n",
    "        # Later comment: wondering when observation_value would be a single value? if obs_val means the vector of observed states in an IPD?\n",
    "        obs_type = self.observation_type\n",
    "        focused_value = (\n",
    "            self.observation_value\n",
    "            if isinstance(self.observation_value, list)\n",
    "            else [self.observation_value]\n",
    "        )\n",
    "        for agent_index in range(self.n_agents):\n",
    "            self._apply_observation_configuration(\n",
    "                agent_index, obs_type, focused_value[0], observations_iso\n",
    "            )\n",
    "\n",
    "    return observations_iso\n",
    "\n",
    "\n",
    "@patch\n",
    "def _apply_observation_configuration(\n",
    "    self: HeterogeneousObservationsEnv,\n",
    "    agent_index,\n",
    "    obs_type,\n",
    "    focused_value,\n",
    "    observations_iso,\n",
    "):\n",
    "    '''Later comment: this function is filling up the observation tensors with different\n",
    "     probabilities for each agent. I'm not sure why I was interested in this kind of shape.\n",
    "     But I guess they represent different weird ways of not observing something'''\n",
    "    if obs_type == \"default\":\n",
    "        for state in range(self.n_states):\n",
    "            if self.n_possible_observations > 1:\n",
    "                remaining_value = (1 - focused_value) / (\n",
    "                    self.n_possible_observations - 1\n",
    "                )\n",
    "            else:\n",
    "                remaining_value = 0.0\n",
    "            observations_iso[agent_index, state, :] = remaining_value\n",
    "            observations_iso[agent_index, state, state] = focused_value\n",
    "    elif obs_type == \"diagonal_confidence\":\n",
    "        for state in range(self.n_states):\n",
    "            observations_iso[agent_index, state, state] = focused_value\n",
    "    elif obs_type == \"fill\":\n",
    "        observations_iso[agent_index, :, :] = focused_value\n",
    "\n",
    "\n",
    "@patch\n",
    "def generate_final_states(self: HeterogeneousObservationsEnv):\n",
    "    \"\"\"Default final states: no final states\"\"\"\n",
    "    return np.zeros(self.n_states, dtype=int)\n",
    "\n",
    "\n",
    "@patch\n",
    "def actions(self: HeterogeneousObservationsEnv):\n",
    "    \"\"\"Default action set representations.\"\"\"\n",
    "    return [[str(a) for a in range(self.n_agent_actions)] for _ in range(self.n_agents)]\n",
    "\n",
    "\n",
    "@patch\n",
    "def states(self: HeterogeneousObservationsEnv):\n",
    "    \"\"\"Default state set representation.\"\"\"\n",
    "    return [str(s) for s in range(self.n_states)]\n",
    "\n",
    "\n",
    "@patch\n",
    "def generate_observation_set(self: HeterogeneousObservationsEnv):\n",
    "    \"\"\"Creates observation labels.\"\"\"\n",
    "    # Currently this only generates labels for games with 1 state: '.'\n",
    "    return [\n",
    "        [str(\".\") for o in range(self.n_possible_observations)]\n",
    "        for _ in range(self.n_agents)\n",
    "    ]\n",
    "\n",
    "\n",
    "@patch\n",
    "def step(self: HeterogeneousObservationsEnv, jA: Iterable) -> tuple:\n",
    "    \"\"\"Iterate the environment one step forward.\"\"\"\n",
    "    tps = self.transitions[tuple([self.state] + list(jA))].astype(float)\n",
    "    next_state = np.random.choice(range(len(tps)), p=tps)\n",
    "    rewards = self.rewards[\n",
    "        tuple([slice(self.n_agents), self.state] + list(jA) + [next_state])\n",
    "    ]\n",
    "    self.state = next_state\n",
    "    # obs = self.generate_stochastic_observations()\n",
    "    # Later comment: the above line has a bug, I think, the observations\n",
    "    # were being set to 0 every time a new step was taken?\n",
    "    obs = self.pick_random_obs_given_state()\n",
    "    # I think the above will work, but I don't understand\n",
    "    # why I'm grabbing a random state to do this with\n",
    "    \n",
    "    done = self.state in np.where(self.final_states == 1)[0]\n",
    "    info = {\"state\": self.state}\n",
    "    return obs, rewards.astype(float), done, info\n",
    "\n",
    "\n",
    "@patch\n",
    "def pick_random_obs_given_state(self: HeterogeneousObservationsEnv) -> np.ndarray:\n",
    "    final_obs = np.zeros(self.n_agents, dtype=int)\n",
    "    for agent in range(self.n_agents):\n",
    "        obs_prob_dist = self.observations[agent, self.state]\n",
    "        obs = np.random.choice(range(len(obs_prob_dist)), p=obs_prob_dist)\n",
    "        final_obs = obs\n",
    "    return final_obs\n",
    "\n",
    "@patch\n",
    "def generate_stochastic_observations(self: HeterogeneousObservationsEnv) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Produces a set of observations for each agent based on the current state, utilizing the defined observation tensors.\n",
    "    Each tensor represents a different observation model, and this method generates observations according to the probability\n",
    "    distributions specified in those tensors for the current state.\n",
    "\n",
    "    Returns:\n",
    "        A list of numpy arrays, where each array contains observations for all agents as determined by one of the observation tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    all_agents_observations = (\n",
    "        []\n",
    "    )  # Stores observations generated by each observation tensor.\n",
    "    for observation_tensor in self.observations_list:\n",
    "        current_state_observations = np.zeros(\n",
    "            self.n_agents, dtype=int\n",
    "        )  # Initializes the observation array for this tensor.\n",
    "        for agent_index in range(self.n_agents):\n",
    "            # Retrieves the probability distribution of observations for the current agent and state from the tensor.\n",
    "            observation_probabilities = observation_tensor[agent_index, self.state]\n",
    "            # Generates a random observation based on the probability distribution.\n",
    "            chosen_observation = np.random.choice(\n",
    "                range(len(observation_probabilities)), p=observation_probabilities\n",
    "            )\n",
    "            current_state_observations[agent_index] = chosen_observation\n",
    "        all_agents_observations.append(current_state_observations)\n",
    "    return all_agents_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d88f0ba2-efc5-4c18-9bc9-0d9c8a253f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
